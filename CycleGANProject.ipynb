{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CycleGAN Implementation\n",
        "\n",
        "**Introduction**\n",
        "\n",
        "Following notebook contains implementation of CycleGAN with brief discriptions of arcitecthure and objective functions. More information could be found in the report. "
      ],
      "metadata": {
        "id": "Jaw7yrgShCeY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dd2HkXZzB-GL"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules.instancenorm import InstanceNorm2d\n",
        "import torch \n",
        "import torch.nn as nn \n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "from tqdm import tqdm \n",
        "import numpy as np \n",
        "import math\n",
        "import os\n",
        "import glob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Network Architecture - Generator**\n",
        "\n",
        "Following blocks are described in the paper for the generator. Due to problems with tensors sizes and pytorch, some of the parameters such as stride size are implemented differently. This is noted by a \"=>\"\n",
        "\n",
        "cs7s1 - k: *conv_block_down*\n",
        "\n",
        "*   Convolution, kernel_size=7x7 \n",
        "*   Instance Normalization\n",
        "* ReLU\n",
        "* k filters\n",
        "* stride 1 => 2\n",
        "\n",
        "dk: *conv_block_down*\n",
        "* Convolution, kernel_size=3x3\n",
        "* Instance Normalization\n",
        "* Relu\n",
        "* k filters and stride 2\n",
        "\n",
        "Rk: *residual_block*\n",
        "* Convolution, kernel_size=7x7\n",
        "* input filter channels = output filter channels\n",
        "\n",
        "uk: *conv_block_up*\n",
        "\n",
        "* Fractional-strided convolution kernel_size=3x3 \n",
        "* k filters\n",
        "* stride 1/2 => 1\n",
        "\n",
        "\n",
        "The section below implents the mentioned blocks"
      ],
      "metadata": {
        "id": "x-OyTWs9a_XG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block_down(in_channels, out_channels, activation='relu',**kwargs): \n",
        "  \"\"\"cs7s1 - k: Down sampling conv block  \"\"\"\n",
        "  activations = nn.ModuleDict([\n",
        "              ['lrelu', nn.LeakyReLU()],\n",
        "              ['relu', nn.ReLU()], \n",
        "              ['none', nn.Identity()]\n",
        "  ])\n",
        "\n",
        "  return nn.Sequential(\n",
        "      nn.Conv2d(in_channels, out_channels,padding_mode=\"reflect\", **kwargs), \n",
        "      InstanceNorm2d(out_channels), \n",
        "      activations[activation]\n",
        "  )\n",
        "\n",
        "def conv_block_up(in_channels, out_channels, activation='relu', **kwargs): \n",
        "  \"\"\"uk: Up sampling Conv block \"\"\"\n",
        "  activations = nn.ModuleDict([\n",
        "              ['lrelu', nn.LeakyReLU()],\n",
        "              ['relu', nn.ReLU()], \n",
        "              ['none', nn.Identity()]\n",
        "  ])\n",
        "\n",
        "  return nn.Sequential(\n",
        "    nn.ConvTranspose2d(in_channels, out_channels, **kwargs), \n",
        "    InstanceNorm2d(out_channels), \n",
        "    activations[activation]\n",
        ")\n",
        "\n",
        "def residual_block(channels=256, **kwargs):\n",
        "  \"\"\"Rx: Residual block. Last layer without activation function  \"\"\"\n",
        "  return nn.Sequential(\n",
        "      conv_block_down(channels, channels, **kwargs),\n",
        "      conv_block_down(channels, channels, activation='none', **kwargs)\n",
        "  )\n"
      ],
      "metadata": {
        "id": "inHH-YdwgorB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From the paper:** the network consists of:\n",
        "c7s1-64,\n",
        "\n",
        "d128,d256,\n",
        "\n",
        "R256,R256,R256,\n",
        "R256,R256,R256,R256,R256,R256,\n",
        "\n",
        "u128, u64,\n",
        "\n",
        "c7s1-3"
      ],
      "metadata": {
        "id": "g28QIdQVcck5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module): \n",
        "  \"\"\" Class for generator  \"\"\"\n",
        "  def __init__(self, in_channels, u_net_sizes = [64, 128, 256], u_net_sizes_R = [256, 128,64],  num_residuals=9): \n",
        "    super().__init__()\n",
        "    self.u_net_sizes = u_net_sizes # channels of the conv down sampling \n",
        "    self.u_net_sizes_R = u_net_sizes_R # channels of conv upsampling \n",
        "\n",
        "    self.initial = nn.Sequential(conv_block_down(in_channels, self.u_net_sizes[0], kernel_size=7, stride=1, padding=3))\n",
        "\n",
        "    self.down_blocks = nn.Sequential(*[conv_block_down(in_c, out_c, kernel_size=3, padding=1, stride=2) # conv down sampling \n",
        "                for in_c, out_c in zip(self.u_net_sizes[0:], self.u_net_sizes[1:])])\n",
        "  \n",
        "    self.res_blocks = nn.ModuleList([residual_block(kernel_size=3, stride=1, padding=1 ) for _ in range(num_residuals)]) # residual blocks \n",
        "\n",
        "    self.up_blocks = nn.Sequential(*[conv_block_up(in_c, out_c, kernel_size=3, stride= 2,padding =1, output_padding=1)\n",
        "                for in_c, out_c in zip(self.u_net_sizes_R[0:], self.u_net_sizes_R[1:])])\n",
        "\n",
        "    self.last = nn.Sequential(conv_block_down(self.u_net_sizes[0], 3, kernel_size=7, padding = 3, stride=1))\n",
        "\n",
        "  def forward(self, x): \n",
        "    x = self.initial(x)\n",
        "    x = self.down_blocks(x)\n",
        "    for block in self.res_blocks: \n",
        "      shortcut = x\n",
        "      x = block(x) + shortcut # Adding shortcut in residual block\n",
        "    x = self.up_blocks(x)\n",
        "    return self.last(x)\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "UJu1CLlSlQ1q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "  img_channels = 3\n",
        "  img_size = 256\n",
        "  x = torch.randn((2, img_channels, img_size, img_size)) # Replicated 2 samples, img_channels in channels and 256x256 image?\n",
        "\n",
        "  gen = Generator(img_channels)\n",
        "  print(gen)\n",
        "  #print(x.size())\n",
        "  pred = gen(x)\n",
        "  print(pred.shape) # 70x 70 patch => each value in the grid sees a 70x70 patch in the orginal image \n",
        "#test()"
      ],
      "metadata": {
        "id": "kHMwGtxGGFG8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Network Architectures - Discriminator**\n",
        "\n",
        "For discriminator networks, we use 70 × 70 PatchGAN.\n",
        "\n",
        "Paper builds the network consisting of following blocks\n",
        "\n",
        "Ck: \n",
        "* Convolution, kernel_size=4x4\n",
        "* InstanceNorm \n",
        "* LeakyReLU, slope 0.2\n",
        "* k filters, stride 2\n",
        "\n",
        "After last layer apply conv to produce a 1-dimensional output. No InstanceNorm for first C64 layer. \n",
        "\n",
        "The discriminator architecture is:\n",
        "C64-C128-C256-C512\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YkeqfUuzm62i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block_D(in_channels, out_channels, *args, **kwargs): \n",
        "  \"\"\"ck - k: \n",
        "   Conv block \"\"\"\n",
        "  return nn.Sequential(\n",
        "      nn.Conv2d(in_channels, out_channels, *args, **kwargs), \n",
        "      InstanceNorm2d(out_channels), \n",
        "      nn.ReLU(),\n",
        "  )"
      ],
      "metadata": {
        "id": "83jHzOfPn7LR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module): \n",
        "  def __init__(self, in_channels, dec_sizes = [64, 128, 256, 512]): \n",
        "    super().__init__()\n",
        "    self.dec_sizes = [in_channels, *dec_sizes]\n",
        "\n",
        "    self.initial = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, self.dec_sizes[1], kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\"), \n",
        "        nn.LeakyReLU(0.2), \n",
        "    )\n",
        "    \n",
        "    conv_blocks = [conv_block_D(in_c, out_c, kernel_size=4,stride=2 if out_c != self.dec_sizes[-1] else 1, padding=1)\n",
        "                for in_c, out_c in zip(self.dec_sizes[1:], self.dec_sizes[2:])]\n",
        "\n",
        "    conv_blocks.append(nn.Conv2d(dec_sizes[-1], 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"))\n",
        "    \n",
        "    self.model = nn.Sequential(*conv_blocks)\n",
        "\n",
        "  def forward(self, x): \n",
        "    x = self.initial(x)\n",
        "    return self.model(x)\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "kpytK03amiex"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "  x = torch.randn((5,3,256,256)) # Replicated 5 samples, 3 in channels and 256x256 image?\n",
        "\n",
        "  model = Discriminator(in_channels=3)\n",
        "  print(model)\n",
        "  pred = model(x)\n",
        "  print(pred.shape) # 70x 70 patch => each value in the grid sees a 70x70 patch in the orginal image \n",
        "\n",
        "#test()"
      ],
      "metadata": {
        "id": "H8f1YeO5s3Mk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset and DataLoader"
      ],
      "metadata": {
        "id": "9NKLRhomxzZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install Pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3I4K4FSz-sd",
        "outputId": "38f53b0d-b568-4d1c-ad5c-a914336de1d8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMcGpmTHxy3B",
        "outputId": "74283733-177b-4c18-f60f-d0caa95c0ec7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset** class for loading and appling basic transformation. Dataset consiting of trainA, trainB, testA and testB must be stored within a folder named '/dataset/' on drive. "
      ],
      "metadata": {
        "id": "dgMN4L9tddw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HorseZebraDataset(Dataset): \n",
        "  \"\"\"\n",
        "  parameter mode: specifices train or test. \n",
        "  glob module retrive all matchign pathnames with specified pattern. \n",
        "  b = zebra, a = horse\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, mode=\"train\", transform=None, root_dir='/content'):\n",
        "    self.root_dir = root_dir\n",
        "    self.mode = mode\n",
        "    self.transform = transform\n",
        "  \n",
        "    self.modeA = glob.glob(os.path.join(self.root_dir,'**/dataset', \"%sA\" % mode, '*.jpg'), \n",
        "                      recursive = True)\n",
        "    self.modeB = glob.glob(os.path.join(self.root_dir,'**/dataset', \"%sB\" % mode, '*.jpg'), \n",
        "                      recursive = True)\n",
        "    self.length_dataset = max(len(self.modeA), len(self.modeB))\n",
        "    self.zebra_len = len(self.modeA)\n",
        "    self.horse_len = len(self.modeB)\n",
        "     \n",
        "    \n",
        "  def __getitem__(self, index):\n",
        "    \"\"\" Modulus operation ensures index not out of bound\"\"\" \n",
        "    idxZ = index % self.zebra_len\n",
        "    idxH = index % self.zebra_len\n",
        "    zebra_img = self.modeB[idxZ]\n",
        "    horse_img = self.modeA[idxH]\n",
        "\n",
        "    zebra_img = np.array(Image.open(zebra_img).convert(\"RGB\"))\n",
        "    horse_img = np.array(Image.open(horse_img).convert(\"RGB\"))\n",
        "\n",
        "    if self.transform: \n",
        "      zebra_img = self.transform(zebra_img)\n",
        "      horse_img = self.transform(horse_img)\n",
        "      \n",
        "\n",
        "    return zebra_img, horse_img\n",
        "\n",
        "  def __len__(self): \n",
        "    # len(dataset)\n",
        "    return self.length_dataset \n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "l1JX5K7i1LAZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "BK5fnU8J5QgI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 1e-5\n",
        "BATCH_SIZE = 1\n",
        "LAMBDA_IDENTITY = 0.0\n",
        "LAMBDA_CYCLE = 10\n",
        "NUM_WORKERS = 4\n",
        "NUM_EPOCHS = 10\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "disc_H = Discriminator(in_channels=3).to(DEVICE)\n",
        "disc_Z = Discriminator(in_channels=3).to(DEVICE)\n",
        "gen_Z = Generator(in_channels=3, num_residuals=9).to(DEVICE)\n",
        "gen_H = Generator(in_channels=3, num_residuals=9).to(DEVICE)\n",
        "\n",
        "transformsA = transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            #transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])    \n",
        "        ]\n",
        "    )\n",
        "\n",
        "opt_disc = torch.optim.Adam(\n",
        "    list(disc_H.parameters()) + list(disc_Z.parameters()),\n",
        "    lr= LEARNING_RATE, \n",
        "    betas=(0.5,0.999)\n",
        ")\n",
        "\n",
        "opt_gen = torch.optim.Adam(\n",
        "    list(gen_Z.parameters()) + list(gen_H.parameters()),\n",
        "    lr= LEARNING_RATE, \n",
        "    betas=(0.5,0.999)\n",
        ")\n",
        "\n",
        "L1 = nn.L1Loss()\n",
        "mse = nn.MSELoss()\n",
        "\n",
        "datasetTrain = HorseZebraDataset(mode=\"train\", transform =transformsA)\n",
        "datasetTest = HorseZebraDataset(mode=\"test\", transform =transformsA)\n",
        "\n",
        "dataLoaderTrain = DataLoader(datasetTrain, batch_size= BATCH_SIZE, shuffle=True)\n",
        "dataLoaderTest = DataLoader(datasetTest,batch_size= BATCH_SIZE, shuffle=True)\n",
        "\n",
        "g_scaler = torch.cuda.amp.GradScaler() # Scale gradients, so they aren't flushed to zero. \"Solves\" problem of too small gradients take into account\n",
        "d_scaler = torch.cuda.amp.GradScaler()"
      ],
      "metadata": {
        "id": "LvYW5BTl5Dqi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"): \n",
        "  print(\"=> saving checkpoint\")\n",
        "  torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(checkpoint): \n",
        "  print(\"=> loading checkpoint\")\n",
        "  gen_Z.load_state_dict(checkpoint['state_dict'])\n",
        "  opt_gen.load_state_dict(checkpoint['optimizer'])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jnzP92DQ7LOJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Train implementaion inspired from https://github.com/aladdinpersson/Machine-Learning-Collection Credits to: Aladdin Persson \"\"\"\n",
        "\n",
        "def train_fn(disc_H, disc_Z, gen_Z, gen_H, loader, opt_disc, opt_gen, l1, mse, d_scalar, g_scalar, epoch ): \n",
        "  \n",
        "  # H_reals = 0\n",
        "  # H_fakes = 0\n",
        "  loop = tqdm(loader, total=len(loader), position=0, leave=True) # wrap loader for progress bar\n",
        "\n",
        "  checkpoint = {'state_dict' : gen_Z.state_dict(), 'optimizer': opt_gen.state_dict()}\n",
        "  save_checkpoint(checkpoint)\n",
        "\n",
        "  for idx, (zebra, horse) in enumerate(loop):\n",
        "    zebra = zebra.to(DEVICE)\n",
        "    horse = horse.to(DEVICE)\n",
        "\n",
        "    # Train Discriminators H and Z\n",
        "\n",
        "    with torch.cuda.amp.autocast(): # automatically cast tensors to a smaller memory footprint, float16\n",
        "      fake_horse = gen_H(zebra) # Generate fake horse from real zebra\n",
        "      D_H_real = disc_H(horse) #  Prediction on the real horses => want it to be 1\n",
        "      D_H_fake = disc_H(fake_horse.detach()) # Prediction of fake horse => want it to be 0. (Detach fake_horse output from the auto_grad-graph as we will use it later)¨\n",
        "      # H_reals += D_H_real.mean().item()\n",
        "      # H_fakes += D_H_fake.mean().item()\n",
        "      D_H_real_loss = mse(D_H_real, torch.ones_like(D_H_real)) # D_H_real goal output is 1, therefor comparing against 1\n",
        "      D_H_fake_loss = mse(D_H_fake, torch.zeros_like(D_H_fake)) # D_H_fake Should output 0\n",
        "      D_H_loss = D_H_real_loss + D_H_fake_loss\n",
        "\n",
        "      fake_zebra = gen_Z(horse)\n",
        "      D_Z_real = disc_Z(zebra) # D_Z_real want it => 1\n",
        "      D_Z_fake = disc_Z(fake_zebra.detach())\n",
        "      D_Z_real_loss = mse(D_Z_real, torch.ones_like(D_H_real)) # D_H_real should output 1\n",
        "      D_Z_fake_loss = mse(D_Z_fake, torch.zeros_like(D_H_fake)) \n",
        "      D_Z_loss = D_Z_real_loss + D_Z_fake_loss\n",
        "\n",
        "      D_loss = D_H_loss + D_Z_loss\n",
        "\n",
        "    opt_disc.zero_grad()\n",
        "    d_scaler.scale(D_loss).backward()\n",
        "    d_scaler.step(opt_disc)\n",
        "    d_scaler.update()\n",
        "\n",
        "\n",
        "    # Train Generator H and Z\n",
        "\n",
        "    with torch.cuda.amp.autocast(): \n",
        "      # adversial loss for both generators\n",
        "      D_H_fake = disc_H(fake_horse) # should output 0\n",
        "      D_Z_fake = disc_Z(fake_zebra) # should output 0\n",
        "      loss_G_H = mse(D_H_fake, torch.ones_like(D_H_fake)) # Want to trick Discrimiator to belive it's real \n",
        "      loss_G_Z = mse(D_Z_fake, torch.ones_like(D_Z_fake)) # Want to trick Discrimiator to belive it's real \n",
        "\n",
        "      # cycle loss \n",
        "      cycle_zebra = gen_Z(fake_horse)\n",
        "      cycle_horse = gen_H(fake_zebra)\n",
        "      cycle_zebra_loss = l1(zebra, cycle_zebra)\n",
        "      cycle_horse_loss = l1(zebra, cycle_horse)\n",
        "\n",
        "      G_loss = (\n",
        "          loss_G_H\n",
        "          + loss_G_Z\n",
        "          + cycle_zebra_loss*LAMBDA_CYCLE\n",
        "          + cycle_horse_loss*LAMBDA_CYCLE\n",
        "      )\n",
        "\n",
        "    opt_gen.zero_grad()\n",
        "    g_scaler.scale(G_loss).backward()\n",
        "    g_scaler.step(opt_gen)\n",
        "    g_scaler.update()\n",
        "\n",
        "    loop.set_description(f\"Epoch [{epoch}/{NUM_EPOCHS}]\") \n",
        "    #loop.set_postfix(H_real=H_reals / (idx + 1), H_fake=H_fakes / (idx + 1))\n",
        "\n",
        "\n",
        "    \n",
        "    if idx % 1200 == 0:\n",
        "      pathA = os.path.join('/content', 'epoch%i'%epoch)\n",
        "      if os.path.exists(pathA) == False:\n",
        "        os.mkdir(pathA)\n",
        "      save_image(fake_horse, f\"{pathA}/horseFake_{idx}.png\")\n",
        "      save_image(horse, f\"{pathA}/horse_{idx}.png\")\n",
        "      save_image(fake_zebra, f\"{pathA}/zebraFake_{idx}.png\")\n",
        "      save_image(zebra, f\"{pathA}/zebra_{idx}.png\")\n",
        "\n"
      ],
      "metadata": {
        "id": "wopX_MhP49OE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    for epoch in range(NUM_EPOCHS):\n",
        "        train_fn(\n",
        "            disc_H,\n",
        "            disc_Z,\n",
        "            gen_Z,\n",
        "            gen_H,\n",
        "            dataLoaderTrain,\n",
        "            opt_disc,\n",
        "            opt_gen,\n",
        "            L1,\n",
        "            mse,\n",
        "            d_scaler,\n",
        "            g_scaler,\n",
        "            epoch\n",
        "        )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfpOcFCc69Rt",
        "outputId": "540117f7-b540-45b9-b09e-48a1936c4d1b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1334 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [0/10]: 100%|██████████| 1334/1334 [08:21<00:00,  2.66it/s]\n",
            "  0%|          | 0/1334 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [1/10]: 100%|██████████| 1334/1334 [03:58<00:00,  5.59it/s]\n",
            "  0%|          | 0/1334 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [2/10]: 100%|██████████| 1334/1334 [03:56<00:00,  5.63it/s]\n",
            "  0%|          | 0/1334 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [3/10]: 100%|██████████| 1334/1334 [03:56<00:00,  5.63it/s]\n",
            "  0%|          | 0/1334 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [4/10]: 100%|██████████| 1334/1334 [03:57<00:00,  5.62it/s]\n",
            "  0%|          | 0/1334 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [5/10]: 100%|██████████| 1334/1334 [03:55<00:00,  5.67it/s]\n",
            "  0%|          | 0/1334 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [6/10]: 100%|██████████| 1334/1334 [03:54<00:00,  5.70it/s]\n",
            "  0%|          | 0/1334 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [7/10]: 100%|██████████| 1334/1334 [03:53<00:00,  5.72it/s]\n",
            "  0%|          | 0/1334 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [8/10]: 100%|██████████| 1334/1334 [03:53<00:00,  5.71it/s]\n",
            "  0%|          | 0/1334 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [9/10]: 100%|██████████| 1334/1334 [03:52<00:00,  5.73it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "COZ5e0tWwWpM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}